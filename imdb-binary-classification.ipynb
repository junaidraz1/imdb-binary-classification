{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf63df5-54d1-45cd-950e-d99436aa4d6c",
   "metadata": {},
   "source": [
    "# Author\n",
    "Muhammad Junaid Raza 2409917"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34bd741-b062-4821-b9ad-169a2f5c116f",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e5601c3-9fe0-4fd8-af1d-959fa09ec17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import random\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005dceed-a90f-439d-8e0f-b8f2cdc8d34d",
   "metadata": {},
   "source": [
    "### Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6fd41ac-86b8-470b-8f6e-8cb11ee597c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reference:\n",
    "### https://stackoverflow.com/questions/57748687/downloading-files-in-jupyter-wget-on-windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44831006-ad5f-434b-8f62-f26a503767a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded imdb-positives.txt\n",
      "Downloaded imdb-negatives.txt\n"
     ]
    }
   ],
   "source": [
    "# List of URLs to download (positive and negative IMDB reviews)\n",
    "urls = [\n",
    "    \"http://dl.turkunlp.org/TKO_7095_2023/imdb-positives.txt\",\n",
    "    \"http://dl.turkunlp.org/TKO_7095_2023/imdb-negatives.txt\",\n",
    "]\n",
    "\n",
    "# Loop through each URL and download the file\n",
    "for url in urls:\n",
    "    wget.download(url)  # Download the file from the URL\n",
    "    print(f\"Downloaded {url.split('/')[-1]}\")  # Print confirmation message with file name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838a92e5-a613-4bc0-9077-5eac11050dfd",
   "metadata": {},
   "source": [
    "### Cleaning and loading the data from downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e99832f5-c7f1-40ba-b688-3e757fab7048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive reviews: 25000\n",
      "Negative reviews: 25000\n"
     ]
    }
   ],
   "source": [
    "# Function to clean text by replacing HTML line breaks with actual newlines\n",
    "def clean_text(text):\n",
    "    return text.replace(\"<br /><br />\", \"\\n\")  # Replace HTML <br /> tags with a newline\n",
    "\n",
    "# Read positive reviews from the file\n",
    "with open(\"imdb-positives.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    positive_reviews = [clean_text(line.strip()) for line in f.readlines()]  # Read and clean each line\n",
    "\n",
    "# Read negative reviews from the file\n",
    "with open(\"imdb-negatives.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    negative_reviews = [clean_text(line.strip()) for line in f.readlines()]  # Read and clean each line\n",
    "\n",
    "print(f\"Positive reviews: {len(positive_reviews)}\")\n",
    "print(f\"Negative reviews: {len(negative_reviews)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990cc1ab-d7b4-407b-b75d-69b86111c283",
   "metadata": {},
   "source": [
    "### Assigning Labels and Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5da9ea1c-0707-4ba5-9f36-900ba694b985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data: 50000\n"
     ]
    }
   ],
   "source": [
    "# Creating labeled dataset\n",
    "positive_data = [{\"text\": text, \"label\": \"positive\"} for text in positive_reviews]\n",
    "negative_data = [{\"text\": text, \"label\": \"negative\"} for text in negative_reviews]\n",
    "\n",
    "# Merge and shuffle the dataset\n",
    "all_data = positive_data + negative_data\n",
    "random.shuffle(all_data) \n",
    "\n",
    "print(f\"Total Data: {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced3249-08ce-4205-b928-be26b84c3a1a",
   "metadata": {},
   "source": [
    "### Spliting the Dataset (80% Train, 10% Validation, 10% Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e96b0982-9d20-44b9-b4ce-059b6597c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate split sizes\n",
    "total_size = len(all_data)\n",
    "train_size = int(0.8 * total_size)\n",
    "valid_size = int(0.1 * total_size)\n",
    "\n",
    "# Perform splitting\n",
    "train_data = all_data[:train_size]\n",
    "valid_data = all_data[train_size : train_size + valid_size]\n",
    "test_data = all_data[train_size + valid_size :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae05788-105e-4418-b92d-d54d9d270c7c",
   "metadata": {},
   "source": [
    "### Converting to DatasetDict Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "51697ad6-adaa-4dc3-940f-fd6fa85dd7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 40000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_data),\n",
    "    \"validation\": Dataset.from_list(valid_data),\n",
    "    \"test\": Dataset.from_list(test_data),\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb167cb-3cdd-481d-832b-d3cdd8b17d8d",
   "metadata": {},
   "source": [
    "### Saving datasets as CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "972ceb9e-f910-4cc3-9500-9e8dd9c9ecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved as CSV files in the current folder.\n"
     ]
    }
   ],
   "source": [
    "# Convert dataset splits to DataFrame\n",
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "valid_df = pd.DataFrame(dataset[\"validation\"])\n",
    "test_df = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "# Saving files as CSV\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "valid_df.to_csv(\"validation.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(\"Datasets saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
